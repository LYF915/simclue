No sentence-transformers model found with name /nfs/users/luoyifei/.cache/torch/sentence_transformers/bert-base-chinese. Creating a new one with MEAN pooling.
No sentence-transformers model found with name /nfs/users/luoyifei/.cache/torch/sentence_transformers/bert-base-chinese. Creating a new one with MEAN pooling.
No sentence-transformers model found with name /nfs/users/luoyifei/.cache/torch/sentence_transformers/bert-base-chinese. Creating a new one with MEAN pooling.
No sentence-transformers model found with name /nfs/users/luoyifei/.cache/torch/sentence_transformers/bert-base-chinese. Creating a new one with MEAN pooling.
No sentence-transformers model found with name /nfs/users/luoyifei/.cache/torch/sentence_transformers/bert-base-chinese. Creating a new one with MEAN pooling.
No sentence-transformers model found with name /nfs/users/luoyifei/.cache/torch/sentence_transformers/bert-base-chinese. Creating a new one with MEAN pooling.
No sentence-transformers model found with name /nfs/users/luoyifei/.cache/torch/sentence_transformers/bert-base-chinese. Creating a new one with MEAN pooling.
Some weights of the model checkpoint at /nfs/users/luoyifei/.cache/torch/sentence_transformers/bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /nfs/users/luoyifei/.cache/torch/sentence_transformers/bert-base-chinese were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /nfs/users/luoyifei/.cache/torch/sentence_transformers/bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /nfs/users/luoyifei/.cache/torch/sentence_transformers/bert-base-chinese were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /nfs/users/luoyifei/.cache/torch/sentence_transformers/bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /nfs/users/luoyifei/.cache/torch/sentence_transformers/bert-base-chinese were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at /nfs/users/luoyifei/.cache/torch/sentence_transformers/bert-base-chinese were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
No sentence-transformers model found with name /nfs/users/luoyifei/.cache/torch/sentence_transformers/bert-base-chinese. Creating a new one with MEAN pooling.
Some weights of the model checkpoint at /nfs/users/luoyifei/.cache/torch/sentence_transformers/bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Epoch:   0%|          | 0/3 [00:00<?, ?it/s]
Epoch:   0%|          | 0/3 [00:00<?, ?it/s]
Epoch:   0%|          | 0/3 [00:00<?, ?it/s]
Epoch:   0%|          | 0/3 [00:00<?, ?it/s]
Epoch:   0%|          | 0/3 [00:00<?, ?it/s]
Epoch:   0%|          | 0/3 [00:00<?, ?it/s]
Epoch:   0%|          | 0/3 [00:00<?, ?it/s]
Iteration:   0%|          | 0/2678728 [00:00<?, ?it/s][A
Iteration:   0%|          | 0/2678728 [00:00<?, ?it/s][A
Epoch:   0%|          | 0/3 [00:00<?, ?it/s]
Iteration:   0%|          | 0/2678728 [00:00<?, ?it/s][A
Iteration:   0%|          | 1/2678728 [00:01<976:06:38,  1.31s/it][A
Iteration:   0%|          | 2/2678728 [00:01<517:33:35,  1.44it/s][A
Iteration:   0%|          | 0/2678728 [00:00<?, ?it/s][A
Iteration:   0%|          | 1/2678728 [00:01<990:59:47,  1.33s/it][A
Iteration:   0%|          | 0/2678728 [00:00<?, ?it/s][A
Iteration:   0%|          | 0/2678728 [00:00<?, ?it/s][A
Iteration:   0%|          | 1/2678728 [00:01<990:45:43,  1.33s/it][A
Iteration:   0%|          | 1/2678728 [00:01<1092:39:57,  1.47s/it][A
Iteration:   0%|          | 3/2678728 [00:01<365:02:17,  2.04it/s][A
Iteration:   0%|          | 2/2678728 [00:01<541:41:28,  1.37it/s][A
Iteration:   0%|          | 1/2678728 [00:01<1089:30:25,  1.46s/it][AIteration:   0%|          | 1/2678728 [00:01<1181:30:26,  1.59s/it]
Epoch:   0%|          | 0/3 [00:01<?, ?it/s]
Traceback (most recent call last):
  File "/cephfs/luoyifei/work/SimCLUE/baselines/finetune_senbert.py", line 65, in <module>
Iteration:   0%|          | 2/2678728 [00:01<592:25:23,  1.26it/s][A
    model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=3, warmup_steps=100, evaluator=evaluator,
  File "/nfs/users/luoyifei/anaconda3/envs/sentencebert/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py", line 722, in fit
    loss_value.backward()
  File "/nfs/users/luoyifei/anaconda3/envs/sentencebert/lib/python3.9/site-packages/torch/tensor.py", line 245, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/nfs/users/luoyifei/anaconda3/envs/sentencebert/lib/python3.9/site-packages/torch/autograd/__init__.py", line 145, in backward
Iteration:   0%|          | 0/2678728 [00:00<?, ?it/s][AIteration:   0%|          | 0/2678728 [00:01<?, ?it/s]
Epoch:   0%|          | 0/3 [00:01<?, ?it/s]
Traceback (most recent call last):
  File "/cephfs/luoyifei/work/SimCLUE/baselines/finetune_senbert.py", line 65, in <module>
    model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=3, warmup_steps=100, evaluator=evaluator,
  File "/nfs/users/luoyifei/anaconda3/envs/sentencebert/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py", line 721, in fit
    loss_value = loss_model(features, labels)
  File "/nfs/users/luoyifei/anaconda3/envs/sentencebert/lib/python3.9/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    Variable._execution_engine.run_backward(
RuntimeError: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.70 GiB total capacity; 1.87 GiB already allocated; 53.56 MiB free; 2.05 GiB reserved in total by PyTorch)
    result = self.forward(*input, **kwargs)
  File "/nfs/users/luoyifei/anaconda3/envs/sentencebert/lib/python3.9/site-packages/sentence_transformers/losses/CosineSimilarityLoss.py", line 39, in forward
    embeddings = [self.model(sentence_feature)['sentence_embedding'] for sentence_feature in sentence_features]
  File "/nfs/users/luoyifei/anaconda3/envs/sentencebert/lib/python3.9/site-packages/sentence_transformers/losses/CosineSimilarityLoss.py", line 39, in <listcomp>
    embeddings = [self.model(sentence_feature)['sentence_embedding'] for sentence_feature in sentence_features]
  File "/nfs/users/luoyifei/anaconda3/envs/sentencebert/lib/python3.9/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/nfs/users/luoyifei/anaconda3/envs/sentencebert/lib/python3.9/site-packages/torch/nn/modules/container.py", line 119, in forward
Iteration:   0%|          | 2/2678728 [00:01<598:32:12,  1.24it/s] [A
    input = module(input)
  File "/nfs/users/luoyifei/anaconda3/envs/sentencebert/lib/python3.9/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/nfs/users/luoyifei/anaconda3/envs/sentencebert/lib/python3.9/site-packages/sentence_transformers/models/Transformer.py", line 66, in forward
    output_states = self.auto_model(**trans_features, return_dict=False)
  File "/nfs/users/luoyifei/anaconda3/envs/sentencebert/lib/python3.9/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/nfs/users/luoyifei/anaconda3/envs/sentencebert/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 1018, in forward
Iteration:   0%|          | 4/2678728 [00:01<303:39:45,  2.45it/s][A
    encoder_outputs = self.encoder(
  File "/nfs/users/luoyifei/anaconda3/envs/sentencebert/lib/python3.9/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/nfs/users/luoyifei/anaconda3/envs/sentencebert/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 607, in forward
    layer_outputs = layer_module(
  File "/nfs/users/luoyifei/anaconda3/envs/sentencebert/lib/python3.9/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/nfs/users/luoyifei/anaconda3/envs/sentencebert/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 493, in forward
    self_attention_outputs = self.attention(
  File "/nfs/users/luoyifei/anaconda3/envs/sentencebert/lib/python3.9/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/nfs/users/luoyifei/anaconda3/envs/sentencebert/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 423, in forward
    self_outputs = self.self(
  File "/nfs/users/luoyifei/anaconda3/envs/sentencebert/lib/python3.9/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/nfs/users/luoyifei/anaconda3/envs/sentencebert/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 289, in forward
    mixed_query_layer = self.query(hidden_states)
  File "/nfs/users/luoyifei/anaconda3/envs/sentencebert/lib/python3.9/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/nfs/users/luoyifei/anaconda3/envs/sentencebert/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 94, in forward
Iteration:   0%|          | 3/2678728 [00:01<418:58:53,  1.78it/s][AIteration:   0%|          | 3/2678728 [00:01<460:45:56,  1.61it/s]
Epoch:   0%|          | 0/3 [00:01<?, ?it/s]
Traceback (most recent call last):
  File "/cephfs/luoyifei/work/SimCLUE/baselines/finetune_senbert.py", line 65, in <module>
    model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=3, warmup_steps=100, evaluator=evaluator,
  File "/nfs/users/luoyifei/anaconda3/envs/sentencebert/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py", line 722, in fit
    loss_value.backward()
  File "/nfs/users/luoyifei/anaconda3/envs/sentencebert/lib/python3.9/site-packages/torch/tensor.py", line 245, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/nfs/users/luoyifei/anaconda3/envs/sentencebert/lib/python3.9/site-packages/torch/autograd/__init__.py", line 145, in backward
    Variable._execution_engine.run_backward(
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 23.70 GiB total capacity; 1.67 GiB already allocated; 15.56 MiB free; 1.82 GiB reserved in total by PyTorch)
    return F.linear(input, self.weight, self.bias)
  File "/nfs/users/luoyifei/anaconda3/envs/sentencebert/lib/python3.9/site-packages/torch/nn/functional.py", line 1753, in linear
Iteration:   0%|          | 1/2678728 [00:01<1075:59:40,  1.45s/it][A
    return torch._C._nn.linear(input, weight, bias)
RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`
Iteration:   0%|          | 3/2678728 [00:01<400:28:44,  1.86it/s][A
Iteration:   0%|          | 3/2678728 [00:01<424:12:08,  1.75it/s][A
Iteration:   0%|          | 5/2678728 [00:01<260:57:33,  2.85it/s][A
Iteration:   0%|          | 2/2678728 [00:01<637:40:55,  1.17it/s] [A
Iteration:   0%|          | 4/2678728 [00:01<331:16:15,  2.25it/s][A
Iteration:   0%|          | 4/2678728 [00:01<340:52:07,  2.18it/s][A
Iteration:   0%|          | 3/2678728 [00:01<446:06:45,  1.67it/s][A
Iteration:   0%|          | 6/2678728 [00:02<231:24:01,  3.22it/s][A
Iteration:   0%|          | 5/2678728 [00:02<289:09:07,  2.57it/s][A
Iteration:   0%|          | 5/2678728 [00:02<286:15:08,  2.60it/s][AIteration:   0%|          | 5/2678728 [00:02<326:37:13,  2.28it/s]
Epoch:   0%|          | 0/3 [00:02<?, ?it/s]
Traceback (most recent call last):
  File "/cephfs/luoyifei/work/SimCLUE/baselines/finetune_senbert.py", line 65, in <module>
    model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=3, warmup_steps=100, evaluator=evaluator,
  File "/nfs/users/luoyifei/anaconda3/envs/sentencebert/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py", line 722, in fit
    loss_value.backward()
  File "/nfs/users/luoyifei/anaconda3/envs/sentencebert/lib/python3.9/site-packages/torch/tensor.py", line 245, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/nfs/users/luoyifei/anaconda3/envs/sentencebert/lib/python3.9/site-packages/torch/autograd/__init__.py", line 145, in backward
    Variable._execution_engine.run_backward(
RuntimeError: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.70 GiB total capacity; 1.89 GiB already allocated; 57.56 MiB free; 2.09 GiB reserved in total by PyTorch)
Iteration:   0%|          | 4/2678728 [00:01<352:36:10,  2.11it/s][A
Iteration:   0%|          | 7/2678728 [00:02<211:06:26,  3.52it/s][A
Iteration:   0%|          | 5/2678728 [00:02<295:28:21,  2.52it/s][AIteration:   0%|          | 5/2678728 [00:02<331:58:24,  2.24it/s]
Epoch:   0%|          | 0/3 [00:02<?, ?it/s]
Traceback (most recent call last):
  File "/cephfs/luoyifei/work/SimCLUE/baselines/finetune_senbert.py", line 65, in <module>
    model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=3, warmup_steps=100, evaluator=evaluator,
  File "/nfs/users/luoyifei/anaconda3/envs/sentencebert/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py", line 722, in fit
    loss_value.backward()
  File "/nfs/users/luoyifei/anaconda3/envs/sentencebert/lib/python3.9/site-packages/torch/tensor.py", line 245, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/nfs/users/luoyifei/anaconda3/envs/sentencebert/lib/python3.9/site-packages/torch/autograd/__init__.py", line 145, in backward
    Variable._execution_engine.run_backward(
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 23.70 GiB total capacity; 1.95 GiB already allocated; 9.56 MiB free; 2.13 GiB reserved in total by PyTorch)
Iteration:   0%|          | 0/2678728 [00:00<?, ?it/s][AIteration:   0%|          | 0/2678728 [00:01<?, ?it/s]
Epoch:   0%|          | 0/3 [00:01<?, ?it/s]
Traceback (most recent call last):
  File "/cephfs/luoyifei/work/SimCLUE/baselines/finetune_senbert.py", line 65, in <module>
Iteration:   0%|          | 6/2678728 [00:02<251:37:28,  2.96it/s][A
    model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=3, warmup_steps=100, evaluator=evaluator,
  File "/nfs/users/luoyifei/anaconda3/envs/sentencebert/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py", line 721, in fit
    loss_value = loss_model(features, labels)
  File "/nfs/users/luoyifei/anaconda3/envs/sentencebert/lib/python3.9/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/nfs/users/luoyifei/anaconda3/envs/sentencebert/lib/python3.9/site-packages/sentence_transformers/losses/CosineSimilarityLoss.py", line 39, in forward
    embeddings = [self.model(sentence_feature)['sentence_embedding'] for sentence_feature in sentence_features]
  File "/nfs/users/luoyifei/anaconda3/envs/sentencebert/lib/python3.9/site-packages/sentence_transformers/losses/CosineSimilarityLoss.py", line 39, in <listcomp>
    embeddings = [self.model(sentence_feature)['sentence_embedding'] for sentence_feature in sentence_features]
  File "/nfs/users/luoyifei/anaconda3/envs/sentencebert/lib/python3.9/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/nfs/users/luoyifei/anaconda3/envs/sentencebert/lib/python3.9/site-packages/torch/nn/modules/container.py", line 119, in forward
    input = module(input)
  File "/nfs/users/luoyifei/anaconda3/envs/sentencebert/lib/python3.9/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/nfs/users/luoyifei/anaconda3/envs/sentencebert/lib/python3.9/site-packages/sentence_transformers/models/Transformer.py", line 66, in forward
    output_states = self.auto_model(**trans_features, return_dict=False)
  File "/nfs/users/luoyifei/anaconda3/envs/sentencebert/lib/python3.9/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/nfs/users/luoyifei/anaconda3/envs/sentencebert/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 1018, in forward
    encoder_outputs = self.encoder(
  File "/nfs/users/luoyifei/anaconda3/envs/sentencebert/lib/python3.9/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/nfs/users/luoyifei/anaconda3/envs/sentencebert/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 607, in forward
    layer_outputs = layer_module(
  File "/nfs/users/luoyifei/anaconda3/envs/sentencebert/lib/python3.9/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/nfs/users/luoyifei/anaconda3/envs/sentencebert/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 493, in forward
    self_attention_outputs = self.attention(
  File "/nfs/users/luoyifei/anaconda3/envs/sentencebert/lib/python3.9/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/nfs/users/luoyifei/anaconda3/envs/sentencebert/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 423, in forward
    self_outputs = self.self(
  File "/nfs/users/luoyifei/anaconda3/envs/sentencebert/lib/python3.9/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
Iteration:   0%|          | 8/2678728 [00:02<193:44:40,  3.84it/s][A
    result = self.forward(*input, **kwargs)
  File "/nfs/users/luoyifei/anaconda3/envs/sentencebert/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 289, in forward
    mixed_query_layer = self.query(hidden_states)
  File "/nfs/users/luoyifei/anaconda3/envs/sentencebert/lib/python3.9/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/nfs/users/luoyifei/anaconda3/envs/sentencebert/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 94, in forward
    return F.linear(input, self.weight, self.bias)
  File "/nfs/users/luoyifei/anaconda3/envs/sentencebert/lib/python3.9/site-packages/torch/nn/functional.py", line 1753, in linear
    return torch._C._nn.linear(input, weight, bias)
RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`
Iteration:   0%|          | 7/2678728 [00:02<224:32:03,  3.31it/s][A
Iteration:   0%|          | 9/2678728 [00:02<179:00:29,  4.16it/s][A
Iteration:   0%|          | 8/2678728 [00:02<203:34:40,  3.66it/s][A
Iteration:   0%|          | 11/2678728 [00:02<156:03:49,  4.77it/s][AIteration:   0%|          | 11/2678728 [00:02<181:51:01,  4.09it/s]
Epoch:   0%|          | 0/3 [00:02<?, ?it/s]
Traceback (most recent call last):
  File "/cephfs/luoyifei/work/SimCLUE/baselines/finetune_senbert.py", line 65, in <module>
    model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=3, warmup_steps=100, evaluator=evaluator,
  File "/nfs/users/luoyifei/anaconda3/envs/sentencebert/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py", line 722, in fit
    loss_value.backward()
  File "/nfs/users/luoyifei/anaconda3/envs/sentencebert/lib/python3.9/site-packages/torch/tensor.py", line 245, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/nfs/users/luoyifei/anaconda3/envs/sentencebert/lib/python3.9/site-packages/torch/autograd/__init__.py", line 145, in backward
    Variable._execution_engine.run_backward(
RuntimeError: CUDA out of memory. Tried to allocate 62.00 MiB (GPU 0; 23.70 GiB total capacity; 1.87 GiB already allocated; 69.56 MiB free; 2.08 GiB reserved in total by PyTorch)
Iteration:   0%|          | 10/2678728 [00:02<171:06:06,  4.35it/s][A
Iteration:   0%|          | 12/2678728 [00:02<150:12:34,  4.95it/s][A
Iteration:   0%|          | 14/2678728 [00:02<136:00:45,  5.47it/s][A
Iteration:   0%|          | 16/2678728 [00:03<124:24:19,  5.98it/s][A
Iteration:   0%|          | 18/2678728 [00:03<115:42:18,  6.43it/s][A
Iteration:   0%|          | 20/2678728 [00:03<109:06:46,  6.82it/s][A
Iteration:   0%|          | 22/2678728 [00:03<103:10:37,  7.21it/s][A
Iteration:   0%|          | 24/2678728 [00:03<98:14:36,  7.57it/s] [A
Iteration:   0%|          | 26/2678728 [00:03<94:40:40,  7.86it/s][A
Iteration:   0%|          | 28/2678728 [00:04<91:28:08,  8.13it/s][A
Iteration:   0%|          | 30/2678728 [00:04<88:26:46,  8.41it/s][A
Iteration:   0%|          | 32/2678728 [00:04<85:38:28,  8.69it/s][A
Iteration:   0%|          | 34/2678728 [00:04<83:31:32,  8.91it/s][A
Iteration:   0%|          | 36/2678728 [00:04<81:08:12,  9.17it/s][A
Iteration:   0%|          | 38/2678728 [00:04<79:26:05,  9.37it/s][A
Iteration:   0%|          | 40/2678728 [00:05<77:58:08,  9.54it/s][A
Iteration:   0%|          | 42/2678728 [00:05<76:54:15,  9.68it/s][A
Iteration:   0%|          | 44/2678728 [00:05<75:34:42,  9.85it/s][A
Iteration:   0%|          | 46/2678728 [00:05<76:11:01,  9.77it/s][A
Iteration:   0%|          | 48/2678728 [00:05<75:35:26,  9.84it/s][A
Iteration:   0%|          | 50/2678728 [00:06<74:56:09,  9.93it/s][A
Iteration:   0%|          | 52/2678728 [00:06<73:50:34, 10.08it/s][A
Iteration:   0%|          | 54/2678728 [00:06<72:41:38, 10.24it/s][A
Iteration:   0%|          | 56/2678728 [00:06<71:45:54, 10.37it/s][A
Iteration:   0%|          | 58/2678728 [00:06<71:09:12, 10.46it/s][A
Iteration:   0%|          | 60/2678728 [00:06<70:38:23, 10.53it/s][A
Iteration:   0%|          | 62/2678728 [00:06<69:50:10, 10.65it/s][A
Iteration:   0%|          | 64/2678728 [00:07<69:12:22, 10.75it/s][A
Iteration:   0%|          | 66/2678728 [00:07<68:27:04, 10.87it/s][A
Iteration:   0%|          | 68/2678728 [00:07<67:53:52, 10.96it/s][A
Iteration:   0%|          | 70/2678728 [00:07<67:02:12, 11.10it/s][A
Iteration:   0%|          | 72/2678728 [00:07<66:32:14, 11.18it/s][A
Iteration:   0%|          | 74/2678728 [00:07<66:01:32, 11.27it/s][A
Iteration:   0%|          | 76/2678728 [00:08<65:15:44, 11.40it/s][A
Iteration:   0%|          | 78/2678728 [00:08<64:43:29, 11.50it/s][A
Iteration:   0%|          | 80/2678728 [00:08<64:54:00, 11.46it/s][A
Iteration:   0%|          | 82/2678728 [00:08<64:27:13, 11.54it/s][A
Iteration:   0%|          | 84/2678728 [00:08<64:05:48, 11.61it/s][A
Iteration:   0%|          | 86/2678728 [00:08<63:57:05, 11.63it/s][A
Iteration:   0%|          | 88/2678728 [00:09<63:58:08, 11.63it/s][A
Iteration:   0%|          | 90/2678728 [00:09<63:35:40, 11.70it/s][A
srun: error: JX-ZY-GPU04: task 1: Exited with exit code 1
Iteration:   0%|          | 92/2678728 [00:09<63:26:38, 11.73it/s][A
srun: error: JX-ZY-GPU04: tasks 0,4: Exited with exit code 1
Iteration:   0%|          | 94/2678728 [00:09<63:33:52, 11.71it/s][A
Iteration:   0%|          | 96/2678728 [00:09<63:25:27, 11.73it/s][A
srun: error: JX-ZY-GPU04: task 6: Exited with exit code 1
srun: error: JX-ZY-GPU04: tasks 2,5: Exited with exit code 1
srun: error: JX-ZY-GPU04: task 7: Exited with exit code 1
Iteration:   0%|          | 98/2678728 [00:09<63:11:35, 11.77it/s][A
Iteration:   0%|          | 98/2678728 [00:19<63:11:35, 11.77it/s][A
Iteration:   0%|          | 100/2678728 [00:44<759:01:40,  1.02s/it][A
Iteration:   0%|          | 102/2678728 [00:44<721:07:09,  1.03it/s][A
Iteration:   0%|          | 104/2678728 [00:44<685:06:17,  1.09it/s][A
Iteration:   0%|          | 106/2678728 [00:44<651:00:45,  1.14it/s][A
Iteration:   0%|          | 108/2678728 [00:45<618:40:30,  1.20it/s][A
Iteration:   0%|          | 110/2678728 [00:45<588:03:34,  1.27it/s][A
Iteration:   0%|          | 112/2678728 [00:45<559:14:49,  1.33it/s][A
Iteration:   0%|          | 114/2678728 [00:45<532:03:21,  1.40it/s][A
Iteration:   0%|          | 116/2678728 [00:45<506:39:16,  1.47it/s][A
Iteration:   0%|          | 118/2678728 [00:45<482:36:11,  1.54it/s][A
Iteration:   0%|          | 120/2678728 [00:45<459:48:47,  1.62it/s][A
Iteration:   0%|          | 122/2678728 [00:45<438:18:40,  1.70it/s][A
Iteration:   0%|          | 124/2678728 [00:46<417:56:03,  1.78it/s][A
Iteration:   0%|          | 126/2678728 [00:46<398:34:43,  1.87it/s][A
Iteration:   0%|          | 128/2678728 [00:46<380:15:06,  1.96it/s][A
Iteration:   0%|          | 130/2678728 [00:46<362:52:13,  2.05it/s][A
Iteration:   0%|          | 132/2678728 [00:46<346:27:39,  2.15it/s][A
Iteration:   0%|          | 134/2678728 [00:46<330:46:24,  2.25it/s][A
Iteration:   0%|          | 136/2678728 [00:46<315:59:04,  2.35it/s][A
Iteration:   0%|          | 138/2678728 [00:46<302:03:37,  2.46it/s][A
Iteration:   0%|          | 140/2678728 [00:47<288:53:53,  2.58it/s][A
Iteration:   0%|          | 142/2678728 [00:47<276:24:00,  2.69it/s][A
Iteration:   0%|          | 144/2678728 [00:47<264:38:42,  2.81it/s][A
Iteration:   0%|          | 146/2678728 [00:47<253:35:38,  2.93it/s][A
Iteration:   0%|          | 148/2678728 [00:47<242:52:55,  3.06it/s][A
Iteration:   0%|          | 150/2678728 [00:47<232:44:48,  3.20it/s][A
Iteration:   0%|          | 152/2678728 [00:47<223:07:57,  3.33it/s][A
Iteration:   0%|          | 154/2678728 [00:47<214:04:09,  3.48it/s][A
Iteration:   0%|          | 156/2678728 [00:48<205:33:48,  3.62it/s][A
Iteration:   0%|          | 158/2678728 [00:48<197:19:46,  3.77it/s][A
Iteration:   0%|          | 160/2678728 [00:48<189:27:20,  3.93it/s][A
Iteration:   0%|          | 162/2678728 [00:48<182:06:40,  4.09it/s][A
Iteration:   0%|          | 164/2678728 [00:48<175:03:06,  4.25it/s][A
Iteration:   0%|          | 166/2678728 [00:48<168:24:13,  4.42it/s][A
Iteration:   0%|          | 168/2678728 [00:48<162:20:50,  4.58it/s][A
Iteration:   0%|          | 170/2678728 [00:48<156:33:04,  4.75it/s][A
Iteration:   0%|          | 172/2678728 [00:48<151:05:12,  4.92it/s][A
Iteration:   0%|          | 174/2678728 [00:49<145:49:39,  5.10it/s][A
Iteration:   0%|          | 176/2678728 [00:49<140:48:41,  5.28it/s][A
Iteration:   0%|          | 178/2678728 [00:49<136:08:30,  5.47it/s][A
Iteration:   0%|          | 180/2678728 [00:49<131:41:27,  5.65it/s][A
Iteration:   0%|          | 182/2678728 [00:49<127:20:08,  5.84it/s][A
Iteration:   0%|          | 184/2678728 [00:49<123:11:41,  6.04it/s][A
Iteration:   0%|          | 186/2678728 [00:49<119:14:57,  6.24it/s][A
Iteration:   0%|          | 188/2678728 [00:49<115:35:47,  6.44it/s][A
Iteration:   0%|          | 190/2678728 [00:50<112:07:22,  6.64it/s][A
Iteration:   0%|          | 192/2678728 [00:50<108:43:05,  6.84it/s][A
Iteration:   0%|          | 194/2678728 [00:50<105:39:46,  7.04it/s][A
Iteration:   0%|          | 196/2678728 [00:50<103:00:43,  7.22it/s][A
Iteration:   0%|          | 198/2678728 [00:50<100:31:14,  7.40it/s][A
Iteration:   0%|          | 198/2678728 [01:10<100:31:14,  7.40it/s][A
Iteration:   0%|          | 200/2678728 [01:24<737:11:54,  1.01it/s][A
Iteration:   0%|          | 202/2678728 [01:25<703:06:04,  1.06it/s][A
Iteration:   0%|          | 204/2678728 [01:25<670:33:27,  1.11it/s][A
Iteration:   0%|          | 206/2678728 [01:25<639:49:16,  1.16it/s][A
Iteration:   0%|          | 208/2678728 [01:25<610:28:34,  1.22it/s][A
Iteration:   0%|          | 210/2678728 [01:25<582:32:09,  1.28it/s][A
Iteration:   0%|          | 212/2678728 [01:25<556:19:09,  1.34it/s][A
Iteration:   0%|          | 214/2678728 [01:26<531:19:07,  1.40it/s][A
Iteration:   0%|          | 216/2678728 [01:26<507:28:53,  1.47it/s][A
Iteration:   0%|          | 218/2678728 [01:26<485:03:31,  1.53it/s][A
Iteration:   0%|          | 220/2678728 [01:26<463:50:59,  1.60it/s][A
Iteration:   0%|          | 222/2678728 [01:26<443:24:59,  1.68it/s][A
Iteration:   0%|          | 224/2678728 [01:26<424:03:37,  1.75it/s][A
Iteration:   0%|          | 226/2678728 [01:26<405:46:30,  1.83it/s][A
Iteration:   0%|          | 228/2678728 [01:27<388:24:20,  1.92it/s][A
Iteration:   0%|          | 230/2678728 [01:27<371:47:37,  2.00it/s][A
Iteration:   0%|          | 232/2678728 [01:27<356:12:09,  2.09it/s][A
Iteration:   0%|          | 234/2678728 [01:27<340:59:52,  2.18it/s][A
Iteration:   0%|          | 236/2678728 [01:27<326:38:53,  2.28it/s][A
Iteration:   0%|          | 238/2678728 [01:27<313:10:09,  2.38it/s][A
Iteration:   0%|          | 240/2678728 [01:28<300:07:21,  2.48it/s][A
Iteration:   0%|          | 242/2678728 [01:28<287:38:55,  2.59it/s][A
Iteration:   0%|          | 244/2678728 [01:28<275:42:45,  2.70it/s][A
Iteration:   0%|          | 246/2678728 [01:28<264:21:50,  2.81it/s][A
Iteration:   0%|          | 248/2678728 [01:28<253:40:29,  2.93it/s][A
Iteration:   0%|          | 250/2678728 [01:28<243:23:13,  3.06it/s][A
Iteration:   0%|          | 252/2678728 [01:28<233:35:33,  3.19it/s][A
Iteration:   0%|          | 254/2678728 [01:28<224:27:53,  3.31it/s][A
Iteration:   0%|          | 256/2678728 [01:29<215:48:11,  3.45it/s][A
Iteration:   0%|          | 258/2678728 [01:29<207:28:56,  3.59it/s][A
Iteration:   0%|          | 260/2678728 [01:29<199:44:12,  3.72it/s][A
Iteration:   0%|          | 262/2678728 [01:29<192:37:14,  3.86it/s][A
Iteration:   0%|          | 264/2678728 [01:29<185:50:31,  4.00it/s][A
Iteration:   0%|          | 266/2678728 [01:29<178:53:23,  4.16it/s][A
Iteration:   0%|          | 268/2678728 [01:29<172:43:07,  4.31it/s][A
Iteration:   0%|          | 270/2678728 [01:30<166:45:24,  4.46it/s][A
Iteration:   0%|          | 272/2678728 [01:30<161:06:35,  4.62it/s][A
Iteration:   0%|          | 274/2678728 [01:30<155:31:33,  4.78it/s][A
Iteration:   0%|          | 276/2678728 [01:30<150:05:20,  4.96it/s][A
Iteration:   0%|          | 278/2678728 [01:30<144:54:38,  5.13it/s][A
Iteration:   0%|          | 280/2678728 [01:30<140:03:08,  5.31it/s][A
Iteration:   0%|          | 282/2678728 [01:30<135:34:00,  5.49it/s][A
Iteration:   0%|          | 284/2678728 [01:31<131:32:41,  5.66it/s][A
Iteration:   0%|          | 286/2678728 [01:31<127:37:39,  5.83it/s][A
Iteration:   0%|          | 288/2678728 [01:31<124:04:01,  6.00it/s][A
Iteration:   0%|          | 290/2678728 [01:31<120:19:47,  6.18it/s][A
Iteration:   0%|          | 292/2678728 [01:31<116:58:56,  6.36it/s][A
Iteration:   0%|          | 294/2678728 [01:31<114:03:28,  6.52it/s][A
Iteration:   0%|          | 296/2678728 [01:31<111:00:00,  6.70it/s][A
Iteration:   0%|          | 298/2678728 [01:32<107:50:32,  6.90it/s][A
Iteration:   0%|          | 298/2678728 [01:50<107:50:32,  6.90it/s][A
Iteration:   0%|          | 300/2678728 [02:05<719:03:06,  1.03it/s][A
Iteration:   0%|          | 302/2678728 [02:05<686:19:25,  1.08it/s][A
Iteration:   0%|          | 304/2678728 [02:05<654:45:14,  1.14it/s][A
Iteration:   0%|          | 306/2678728 [02:05<624:50:51,  1.19it/s][A
Iteration:   0%|          | 308/2678728 [02:05<596:30:24,  1.25it/s][A
Iteration:   0%|          | 310/2678728 [02:05<569:48:08,  1.31it/s][A
Iteration:   0%|          | 312/2678728 [02:06<544:26:51,  1.37it/s][A
Iteration:   0%|          | 314/2678728 [02:06<520:08:03,  1.43it/s][A
Iteration:   0%|          | 316/2678728 [02:06<497:02:21,  1.50it/s][A
Iteration:   0%|          | 318/2678728 [02:06<475:17:48,  1.57it/s][A
Iteration:   0%|          | 320/2678728 [02:06<454:33:33,  1.64it/s][A
Iteration:   0%|          | 322/2678728 [02:06<434:48:35,  1.71it/s][A
Iteration:   0%|          | 324/2678728 [02:07<415:50:59,  1.79it/s][A
Iteration:   0%|          | 326/2678728 [02:07<397:56:19,  1.87it/s][A
Iteration:   0%|          | 328/2678728 [02:07<380:53:15,  1.95it/s][A
Iteration:   0%|          | 330/2678728 [02:07<364:48:49,  2.04it/s][A
Iteration:   0%|          | 332/2678728 [02:07<349:24:40,  2.13it/s][A
Iteration:   0%|          | 334/2678728 [02:07<334:52:51,  2.22it/s][A
Iteration:   0%|          | 336/2678728 [02:08<321:06:47,  2.32it/s][A
Iteration:   0%|          | 338/2678728 [02:08<307:51:52,  2.42it/s][A
Iteration:   0%|          | 340/2678728 [02:08<295:25:14,  2.52it/s][A
Iteration:   0%|          | 342/2678728 [02:08<283:44:50,  2.62it/s][A
Iteration:   0%|          | 344/2678728 [02:08<272:32:38,  2.73it/s][A
Iteration:   0%|          | 346/2678728 [02:08<261:54:52,  2.84it/s][A
Iteration:   0%|          | 348/2678728 [02:08<251:26:59,  2.96it/s][A
Iteration:   0%|          | 350/2678728 [02:09<241:48:26,  3.08it/s][A
Iteration:   0%|          | 352/2678728 [02:09<232:43:41,  3.20it/s][A
Iteration:   0%|          | 354/2678728 [02:09<224:11:49,  3.32it/s][A
Iteration:   0%|          | 356/2678728 [02:09<216:05:12,  3.44it/s][A
Iteration:   0%|          | 358/2678728 [02:09<208:15:04,  3.57it/s][A
Iteration:   0%|          | 360/2678728 [02:09<200:50:03,  3.70it/s][A
Iteration:   0%|          | 362/2678728 [02:10<193:28:53,  3.85it/s][A
Iteration:   0%|          | 364/2678728 [02:10<186:50:14,  3.98it/s][A
Iteration:   0%|          | 366/2678728 [02:10<180:21:02,  4.13it/s][A
Iteration:   0%|          | 368/2678728 [02:10<174:10:02,  4.27it/s][A
Iteration:   0%|          | 370/2678728 [02:10<168:16:48,  4.42it/s][A
Iteration:   0%|          | 372/2678728 [02:10<162:53:53,  4.57it/s][A
Iteration:   0%|          | 374/2678728 [02:11<157:39:05,  4.72it/s][A
Iteration:   0%|          | 376/2678728 [02:11<152:33:17,  4.88it/s][A
Iteration:   0%|          | 378/2678728 [02:11<147:50:40,  5.03it/s][A
Iteration:   0%|          | 380/2678728 [02:11<143:26:52,  5.19it/s][A
Iteration:   0%|          | 382/2678728 [02:11<139:24:42,  5.34it/s][A
Iteration:   0%|          | 384/2678728 [02:11<135:03:16,  5.51it/s][A
Iteration:   0%|          | 386/2678728 [02:11<131:08:19,  5.67it/s][A
Iteration:   0%|          | 388/2678728 [02:12<127:24:14,  5.84it/s][A
Iteration:   0%|          | 390/2678728 [02:12<123:57:12,  6.00it/s][A
Iteration:   0%|          | 392/2678728 [02:12<120:54:01,  6.15it/s][A
Iteration:   0%|          | 394/2678728 [02:12<117:46:44,  6.32it/s][A
Iteration:   0%|          | 396/2678728 [02:12<114:53:17,  6.48it/s][A
Iteration:   0%|          | 398/2678728 [02:12<112:05:40,  6.64it/s][A
Iteration:   0%|          | 398/2678728 [02:30<112:05:40,  6.64it/s][A
Iteration:   0%|          | 400/2678728 [02:44<688:17:26,  1.08it/s][A
Iteration:   0%|          | 402/2678728 [02:44<656:46:07,  1.13it/s][A
Iteration:   0%|          | 404/2678728 [02:44<626:43:31,  1.19it/s][A
Iteration:   0%|          | 406/2678728 [02:44<598:19:06,  1.24it/s][A
Iteration:   0%|          | 408/2678728 [02:44<571:08:40,  1.30it/s][A
Iteration:   0%|          | 410/2678728 [02:44<545:12:32,  1.36it/s][A
Iteration:   0%|          | 412/2678728 [02:45<520:32:13,  1.43it/s][A
Iteration:   0%|          | 414/2678728 [02:45<497:07:36,  1.50it/s][A
Iteration:   0%|          | 416/2678728 [02:45<475:00:09,  1.57it/s][A
Iteration:   0%|          | 418/2678728 [02:45<453:41:03,  1.64it/s][A
Iteration:   0%|          | 420/2678728 [02:45<433:35:41,  1.72it/s][A
Iteration:   0%|          | 422/2678728 [02:45<414:37:41,  1.79it/s][A
Iteration:   0%|          | 424/2678728 [02:45<396:41:49,  1.88it/s][A
Iteration:   0%|          | 426/2678728 [02:46<379:35:39,  1.96it/s][A
Iteration:   0%|          | 428/2678728 [02:46<363:08:37,  2.05it/s][A
Iteration:   0%|          | 430/2678728 [02:46<347:49:40,  2.14it/s][A
Iteration:   0%|          | 432/2678728 [02:46<333:16:54,  2.23it/s][A
Iteration:   0%|          | 434/2678728 [02:46<319:28:10,  2.33it/s][A
Iteration:   0%|          | 436/2678728 [02:46<306:19:34,  2.43it/s][A
Iteration:   0%|          | 438/2678728 [02:47<293:42:12,  2.53it/s][A
Iteration:   0%|          | 440/2678728 [02:47<281:23:45,  2.64it/s][A
Iteration:   0%|          | 442/2678728 [02:47<269:38:05,  2.76it/s][A
Iteration:   0%|          | 444/2678728 [02:47<258:44:46,  2.88it/s][A
Iteration:   0%|          | 446/2678728 [02:47<248:18:04,  3.00it/s][A
Iteration:   0%|          | 448/2678728 [02:47<238:32:29,  3.12it/s][A
Iteration:   0%|          | 450/2678728 [02:47<229:20:29,  3.24it/s][A
Iteration:   0%|          | 452/2678728 [02:47<220:24:41,  3.38it/s][A
Iteration:   0%|          | 454/2678728 [02:48<212:03:00,  3.51it/s][A
Iteration:   0%|          | 456/2678728 [02:48<204:02:57,  3.65it/s][A
Iteration:   0%|          | 458/2678728 [02:48<196:19:25,  3.79it/s][A
Iteration:   0%|          | 460/2678728 [02:48<189:03:35,  3.94it/s][A
Iteration:   0%|          | 462/2678728 [02:48<182:07:59,  4.08it/s][A
Iteration:   0%|          | 464/2678728 [02:48<175:16:55,  4.24it/s][A
Iteration:   0%|          | 466/2678728 [02:48<169:04:02,  4.40it/s][A
Iteration:   0%|          | 468/2678728 [02:49<163:01:17,  4.56it/s][A
Iteration:   0%|          | 470/2678728 [02:49<157:29:17,  4.72it/s][A
Iteration:   0%|          | 472/2678728 [02:49<152:11:22,  4.89it/s][A
Iteration:   0%|          | 474/2678728 [02:49<147:16:06,  5.05it/s][A
Iteration:   0%|          | 476/2678728 [02:49<142:36:44,  5.22it/s][A
Iteration:   0%|          | 478/2678728 [02:49<138:06:52,  5.39it/s][A
Iteration:   0%|          | 480/2678728 [02:49<133:38:47,  5.57it/s][A
Iteration:   0%|          | 482/2678728 [02:50<129:30:30,  5.74it/s][A
Iteration:   0%|          | 484/2678728 [02:50<125:23:01,  5.93it/s][A
Iteration:   0%|          | 486/2678728 [02:50<121:46:29,  6.11it/s][A
Iteration:   0%|          | 488/2678728 [02:50<118:05:52,  6.30it/s][A
Iteration:   0%|          | 490/2678728 [02:50<114:39:22,  6.49it/s][A
Iteration:   0%|          | 492/2678728 [02:50<111:34:07,  6.67it/s][A
Iteration:   0%|          | 494/2678728 [02:50<108:43:12,  6.84it/s][A
Iteration:   0%|          | 496/2678728 [02:50<105:49:17,  7.03it/s][A
Iteration:   0%|          | 498/2678728 [02:51<103:10:43,  7.21it/s][A
Iteration:   0%|          | 498/2678728 [03:10<103:10:43,  7.21it/s][Asrun: error: JX-ZY-GPU04: task 3: Killed
srun: Force Terminated job step 143041.0
